{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:120: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:120: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "WARN: Cannot define MaxPoolGrad, likely already defined for this version of tensorflow: \"Registering two gradient with name 'MaxPoolGrad'! (Previous registration was in register /Users/zhiyulin/opt/anaconda3/envs/anly580/lib/python3.8/site-packages/tensorflow/python/framework/registry.py:66)\"\n",
      "<ipython-input-1-7604991a83df>:120: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if FLAGS.norm is not 'None':\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Code for the MAML algorithm and network definitions. \"\"\"\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "try:\n",
    "    from maml import special_grads\n",
    "except KeyError as e:\n",
    "    print('WARN: Cannot define MaxPoolGrad, likely already defined for this version of tensorflow: %s' % e,\n",
    "          file=sys.stderr)\n",
    "\n",
    "from tensorflow.python.platform import flags\n",
    "from maml.utils import mse, xent, conv_block, normalize\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "class MAML:\n",
    "    def __init__(self, dim_input=1, dim_output=1, test_num_updates=5):\n",
    "        \"\"\" must call construct_model() after initializing MAML! \"\"\"\n",
    "        self.dim_input = dim_input\n",
    "        self.dim_output = dim_output\n",
    "        self.update_lr = FLAGS.update_lr\n",
    "        self.meta_lr = tf.placeholder_with_default(FLAGS.meta_lr, ())\n",
    "        self.classification = False\n",
    "        self.test_num_updates = test_num_updates\n",
    "        if FLAGS.datasource == 'sinusoid':\n",
    "            self.dim_hidden = [40, 40]\n",
    "            self.loss_func = mse\n",
    "            self.forward = self.forward_fc\n",
    "            self.construct_weights = self.construct_fc_weights\n",
    "        elif FLAGS.datasource == 'omniglot' or FLAGS.datasource == 'miniimagenet':\n",
    "            self.loss_func = xent\n",
    "            self.classification = True\n",
    "            if FLAGS.conv:\n",
    "                self.dim_hidden = FLAGS.num_filters\n",
    "                self.forward = self.forward_conv\n",
    "                self.construct_weights = self.construct_conv_weights\n",
    "            else:\n",
    "                self.dim_hidden = [256, 128, 64, 64]\n",
    "                self.forward=self.forward_fc\n",
    "                self.construct_weights = self.construct_fc_weights\n",
    "            if FLAGS.datasource == 'miniimagenet':\n",
    "                self.channels = 3\n",
    "            else:\n",
    "                self.channels = 1\n",
    "            self.img_size = int(np.sqrt(self.dim_input/self.channels))\n",
    "        else:\n",
    "            raise ValueError('Unrecognized data source.')\n",
    "\n",
    "    def construct_model(self, input_tensors=None, prefix='metatrain_'):\n",
    "        # a: training data for inner gradient, b: test data for meta gradient\n",
    "        if input_tensors is None:\n",
    "            self.inputa = tf.placeholder(tf.float32)\n",
    "            self.inputb = tf.placeholder(tf.float32)\n",
    "            self.labela = tf.placeholder(tf.float32)\n",
    "            self.labelb = tf.placeholder(tf.float32)\n",
    "        else:\n",
    "            self.inputa = input_tensors['inputa']\n",
    "            self.inputb = input_tensors['inputb']\n",
    "            self.labela = input_tensors['labela']\n",
    "            self.labelb = input_tensors['labelb']\n",
    "\n",
    "        with tf.variable_scope('model', reuse=None) as training_scope:\n",
    "            if 'weights' in dir(self):\n",
    "                training_scope.reuse_variables()\n",
    "                weights = self.weights\n",
    "            else:\n",
    "                # Define the weights\n",
    "                self.weights = weights = self.construct_weights()\n",
    "\n",
    "            # outputbs[i] and lossesb[i] is the output and loss after i+1 gradient updates\n",
    "            lossesa, outputas, lossesb, outputbs = [], [], [], []\n",
    "            accuraciesa, accuraciesb = [], []\n",
    "            num_updates = max(self.test_num_updates, FLAGS.num_updates)\n",
    "            outputbs = [[]]*num_updates\n",
    "            lossesb = [[]]*num_updates\n",
    "            accuraciesb = [[]]*num_updates\n",
    "\n",
    "            def task_metalearn(inp, reuse=True):\n",
    "                \"\"\" Perform gradient descent for one task in the meta-batch. \"\"\"\n",
    "                inputa, inputb, labela, labelb = inp\n",
    "                task_outputbs, task_lossesb = [], []\n",
    "\n",
    "                if self.classification:\n",
    "                    task_accuraciesb = []\n",
    "\n",
    "                task_outputa = self.forward(inputa, weights, reuse=reuse)  # only reuse on the first iter\n",
    "                task_lossa = self.loss_func(task_outputa, labela)\n",
    "\n",
    "                grads = tf.gradients(task_lossa, list(weights.values()))\n",
    "                if FLAGS.stop_grad:\n",
    "                    grads = [tf.stop_gradient(grad) for grad in grads]\n",
    "                gradients = dict(zip(weights.keys(), grads))\n",
    "                fast_weights = dict(zip(weights.keys(), [weights[key] - self.update_lr*gradients[key] for key in weights.keys()]))\n",
    "                output = self.forward(inputb, fast_weights, reuse=True)\n",
    "                task_outputbs.append(output)\n",
    "                task_lossesb.append(self.loss_func(output, labelb))\n",
    "\n",
    "                for j in range(num_updates - 1):\n",
    "                    loss = self.loss_func(self.forward(inputa, fast_weights, reuse=True), labela)\n",
    "                    grads = tf.gradients(loss, list(fast_weights.values()))\n",
    "                    if FLAGS.stop_grad:\n",
    "                        grads = [tf.stop_gradient(grad) for grad in grads]\n",
    "                    gradients = dict(zip(fast_weights.keys(), grads))\n",
    "                    fast_weights = dict(zip(fast_weights.keys(), [fast_weights[key] - self.update_lr*gradients[key] for key in fast_weights.keys()]))\n",
    "                    output = self.forward(inputb, fast_weights, reuse=True)\n",
    "                    task_outputbs.append(output)\n",
    "                    task_lossesb.append(self.loss_func(output, labelb))\n",
    "\n",
    "                task_output = [task_outputa, task_outputbs, task_lossa, task_lossesb]\n",
    "\n",
    "                if self.classification:\n",
    "                    task_accuracya = tf.keras.metrics.accuracy(tf.argmax(tf.nn.softmax(task_outputa), 1), tf.argmax(labela, 1))\n",
    "                    for j in range(num_updates):\n",
    "                        task_accuraciesb.append(tf.keras.metrics.accuracy(tf.argmax(tf.nn.softmax(task_outputbs[j]), 1), tf.argmax(labelb, 1)))\n",
    "                    task_output.extend([task_accuracya, task_accuraciesb])\n",
    "\n",
    "                return task_output\n",
    "\n",
    "            if FLAGS.norm is not 'None':\n",
    "                # to initialize the batch norm vars, might want to combine this, and not run idx 0 twice.\n",
    "                unused = task_metalearn((self.inputa[0], self.inputb[0], self.labela[0], self.labelb[0]), False)\n",
    "\n",
    "            out_dtype = [tf.float32, [tf.float32]*num_updates, tf.float32, [tf.float32]*num_updates]\n",
    "            if self.classification:\n",
    "                out_dtype.extend([tf.float32, [tf.float32]*num_updates])\n",
    "            result = tf.map_fn(task_metalearn, elems=(self.inputa, self.inputb, self.labela, self.labelb), dtype=out_dtype, parallel_iterations=FLAGS.meta_batch_size)\n",
    "            if self.classification:\n",
    "                outputas, outputbs, lossesa, lossesb, accuraciesa, accuraciesb = result\n",
    "            else:\n",
    "                outputas, outputbs, lossesa, lossesb  = result\n",
    "\n",
    "        ## Performance & Optimization\n",
    "        if 'train' in prefix:\n",
    "            self.total_loss1 = total_loss1 = tf.reduce_sum(lossesa) / tf.to_float(FLAGS.meta_batch_size)\n",
    "            self.total_losses2 = total_losses2 = [tf.reduce_sum(lossesb[j]) / tf.to_float(FLAGS.meta_batch_size) for j in range(num_updates)]\n",
    "            # after the map_fn\n",
    "            self.outputas, self.outputbs = outputas, outputbs\n",
    "            if self.classification:\n",
    "                self.total_accuracy1 = total_accuracy1 = tf.reduce_sum(accuraciesa) / tf.to_float(FLAGS.meta_batch_size)\n",
    "                self.total_accuracies2 = total_accuracies2 = [tf.reduce_sum(accuraciesb[j]) / tf.to_float(FLAGS.meta_batch_size) for j in range(num_updates)]\n",
    "            self.pretrain_op = tf.train.AdamOptimizer(self.meta_lr).minimize(total_loss1)\n",
    "\n",
    "            if FLAGS.metatrain_iterations > 0:\n",
    "                optimizer = tf.train.AdamOptimizer(self.meta_lr)\n",
    "                self.gvs = gvs = optimizer.compute_gradients(self.total_losses2[FLAGS.num_updates-1])\n",
    "                if FLAGS.datasource == 'miniimagenet':\n",
    "                    gvs = [(tf.clip_by_value(grad, -10, 10), var) for grad, var in gvs]\n",
    "                self.metatrain_op = optimizer.apply_gradients(gvs)\n",
    "        else:\n",
    "            self.metaval_total_loss1 = total_loss1 = tf.reduce_sum(lossesa) / tf.to_float(FLAGS.meta_batch_size)\n",
    "            self.metaval_total_losses2 = total_losses2 = [tf.reduce_sum(lossesb[j]) / tf.to_float(FLAGS.meta_batch_size) for j in range(num_updates)]\n",
    "            if self.classification:\n",
    "                self.metaval_total_accuracy1 = total_accuracy1 = tf.reduce_sum(accuraciesa) / tf.to_float(FLAGS.meta_batch_size)\n",
    "                self.metaval_total_accuracies2 = total_accuracies2 =[tf.reduce_sum(accuraciesb[j]) / tf.to_float(FLAGS.meta_batch_size) for j in range(num_updates)]\n",
    "\n",
    "        ## Summaries\n",
    "        tf.summary.scalar(prefix+'Pre-update loss', total_loss1)\n",
    "        if self.classification:\n",
    "            tf.summary.scalar(prefix+'Pre-update accuracy', total_accuracy1)\n",
    "\n",
    "        for j in range(num_updates):\n",
    "            tf.summary.scalar(prefix+'Post-update loss, step ' + str(j+1), total_losses2[j])\n",
    "            if self.classification:\n",
    "                tf.summary.scalar(prefix+'Post-update accuracy, step ' + str(j+1), total_accuracies2[j])\n",
    "\n",
    "    ### Network construction functions (fc networks and conv networks)\n",
    "    def construct_fc_weights(self):\n",
    "        weights = {}\n",
    "        weights['w1'] = tf.Variable(tf.truncated_normal([self.dim_input, self.dim_hidden[0]], stddev=0.01))\n",
    "        weights['b1'] = tf.Variable(tf.zeros([self.dim_hidden[0]]))\n",
    "        for i in range(1,len(self.dim_hidden)):\n",
    "            weights['w'+str(i+1)] = tf.Variable(tf.truncated_normal([self.dim_hidden[i-1], self.dim_hidden[i]], stddev=0.01))\n",
    "            weights['b'+str(i+1)] = tf.Variable(tf.zeros([self.dim_hidden[i]]))\n",
    "        weights['w'+str(len(self.dim_hidden)+1)] = tf.Variable(tf.truncated_normal([self.dim_hidden[-1], self.dim_output], stddev=0.01))\n",
    "        weights['b'+str(len(self.dim_hidden)+1)] = tf.Variable(tf.zeros([self.dim_output]))\n",
    "        return weights\n",
    "\n",
    "    def forward_fc(self, inp, weights, reuse=False):\n",
    "        hidden = normalize(tf.matmul(inp, weights['w1']) + weights['b1'], activation=tf.nn.relu, reuse=reuse, scope='0')\n",
    "        for i in range(1,len(self.dim_hidden)):\n",
    "            hidden = normalize(tf.matmul(hidden, weights['w'+str(i+1)]) + weights['b'+str(i+1)], activation=tf.nn.relu, reuse=reuse, scope=str(i+1))\n",
    "        return tf.matmul(hidden, weights['w'+str(len(self.dim_hidden)+1)]) + weights['b'+str(len(self.dim_hidden)+1)]\n",
    "\n",
    "    def construct_conv_weights(self):\n",
    "        weights = {}\n",
    "\n",
    "        dtype = tf.float32\n",
    "        conv_initializer =  tf.keras.layers.xavier_initializer_conv2d(dtype=dtype)\n",
    "        fc_initializer =  tf.keras.layers.xavier_initializer(dtype=dtype)\n",
    "        k = 3\n",
    "\n",
    "        weights['conv1'] = tf.get_variable('conv1', [k, k, self.channels, self.dim_hidden], initializer=conv_initializer, dtype=dtype)\n",
    "        weights['b1'] = tf.Variable(tf.zeros([self.dim_hidden]))\n",
    "        weights['conv2'] = tf.get_variable('conv2', [k, k, self.dim_hidden, self.dim_hidden], initializer=conv_initializer, dtype=dtype)\n",
    "        weights['b2'] = tf.Variable(tf.zeros([self.dim_hidden]))\n",
    "        weights['conv3'] = tf.get_variable('conv3', [k, k, self.dim_hidden, self.dim_hidden], initializer=conv_initializer, dtype=dtype)\n",
    "        weights['b3'] = tf.Variable(tf.zeros([self.dim_hidden]))\n",
    "        weights['conv4'] = tf.get_variable('conv4', [k, k, self.dim_hidden, self.dim_hidden], initializer=conv_initializer, dtype=dtype)\n",
    "        weights['b4'] = tf.Variable(tf.zeros([self.dim_hidden]))\n",
    "        if FLAGS.datasource == 'miniimagenet':\n",
    "            # assumes max pooling\n",
    "            weights['w5'] = tf.get_variable('w5', [self.dim_hidden*5*5, self.dim_output], initializer=fc_initializer)\n",
    "            weights['b5'] = tf.Variable(tf.zeros([self.dim_output]), name='b5')\n",
    "        else:\n",
    "            weights['w5'] = tf.Variable(tf.random_normal([self.dim_hidden, self.dim_output]), name='w5')\n",
    "            weights['b5'] = tf.Variable(tf.zeros([self.dim_output]), name='b5')\n",
    "        return weights\n",
    "\n",
    "    def forward_conv(self, inp, weights, reuse=False, scope=''):\n",
    "        # reuse is for the normalization parameters.\n",
    "        channels = self.channels\n",
    "        inp = tf.reshape(inp, [-1, self.img_size, self.img_size, channels])\n",
    "\n",
    "        hidden1 = conv_block(inp, weights['conv1'], weights['b1'], reuse, scope+'0')\n",
    "        hidden2 = conv_block(hidden1, weights['conv2'], weights['b2'], reuse, scope+'1')\n",
    "        hidden3 = conv_block(hidden2, weights['conv3'], weights['b3'], reuse, scope+'2')\n",
    "        hidden4 = conv_block(hidden3, weights['conv4'], weights['b4'], reuse, scope+'3')\n",
    "        if FLAGS.datasource == 'miniimagenet':\n",
    "            # last hidden layer is 6x6x64-ish, reshape to a vector\n",
    "            hidden4 = tf.reshape(hidden4, [-1, np.prod([int(dim) for dim in hidden4.get_shape()[1:]])])\n",
    "        else:\n",
    "            hidden4 = tf.reduce_mean(hidden4, [1, 2])\n",
    "\n",
    "        return tf.matmul(hidden4, weights['w5']) + weights['b5']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhiyulin/Downloads/oral_arguments/maml/maml.py:120: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if FLAGS.norm is not 'None':\n",
      "WARN: Cannot define MaxPoolGrad, likely already defined for this version of tensorflow: \"Registering two gradient with name 'MaxPoolGrad'! (Previous registration was in register /Users/zhiyulin/opt/anaconda3/envs/anly580/lib/python3.8/site-packages/tensorflow/python/framework/registry.py:66)\"\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "from maml.data_generator import DataGenerator\n",
    "from maml.maml import MAML\n",
    "from tensorflow.python.platform import flags\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "## Dataset/method options\n",
    "flags.DEFINE_string('datasource', 'sinusoid', 'sinusoid or omniglot or miniimagenet')\n",
    "flags.DEFINE_integer('num_classes', 5, 'number of classes used in classification (e.g. 5-way classification).')\n",
    "# oracle means task id is input (only suitable for sinusoid)\n",
    "flags.DEFINE_string('baseline', None, 'oracle, or None')\n",
    "\n",
    "## Training options\n",
    "flags.DEFINE_integer('pretrain_iterations', 0, 'number of pre-training iterations.')\n",
    "flags.DEFINE_integer('metatrain_iterations', 15000, 'number of metatraining iterations.') # 15k for omniglot, 50k for sinusoid\n",
    "flags.DEFINE_integer('meta_batch_size', 25, 'number of tasks sampled per meta-update')\n",
    "flags.DEFINE_float('meta_lr', 0.001, 'the base learning rate of the generator')\n",
    "flags.DEFINE_integer('update_batch_size', 5, 'number of examples used for inner gradient update (K for K-shot learning).')\n",
    "flags.DEFINE_float('update_lr', 1e-3, 'step size alpha for inner gradient update.') # 0.1 for omniglot\n",
    "flags.DEFINE_integer('num_updates', 1, 'number of inner gradient updates during training.')\n",
    "\n",
    "## Model options\n",
    "flags.DEFINE_string('norm', 'batch_norm', 'batch_norm, layer_norm, or None')\n",
    "flags.DEFINE_integer('num_filters', 64, 'number of filters for conv nets -- 32 for miniimagenet, 64 for omiglot.')\n",
    "flags.DEFINE_bool('conv', True, 'whether or not to use a convolutional network, only applicable in some cases')\n",
    "flags.DEFINE_bool('max_pool', False, 'Whether or not to use max pooling rather than strided convolutions')\n",
    "flags.DEFINE_bool('stop_grad', False, 'if True, do not use second derivatives in meta-optimization (for speed)')\n",
    "\n",
    "## Logging, saving, and testing options\n",
    "flags.DEFINE_bool('log', True, 'if false, do not log summaries, for debugging code.')\n",
    "flags.DEFINE_string('logdir', '/tmp/data', 'directory for summaries and checkpoints.')\n",
    "flags.DEFINE_bool('resume', True, 'resume training if there is a model available')\n",
    "flags.DEFINE_bool('train', True, 'True to train, False to test.')\n",
    "flags.DEFINE_integer('test_iter', -1, 'iteration to load model (-1 for latest model)')\n",
    "flags.DEFINE_bool('test_set', False, 'Set to true to test on the the test set, False for the validation set.')\n",
    "flags.DEFINE_integer('train_update_batch_size', -1, 'number of examples used for gradient update during training (use if you want to test with a different number).')\n",
    "flags.DEFINE_float('train_update_lr', -1, 'value of inner gradient step step during training. (use if you want to test with a different value)') # 0.1 for omniglot\n",
    "\n",
    "def train(model, saver, sess, exp_string, data_generator, resume_itr=0):\n",
    "    SUMMARY_INTERVAL = 100\n",
    "    SAVE_INTERVAL = 1000\n",
    "    if FLAGS.datasource == 'sinusoid':\n",
    "        PRINT_INTERVAL = 1000\n",
    "        TEST_PRINT_INTERVAL = PRINT_INTERVAL*5\n",
    "    else:\n",
    "        PRINT_INTERVAL = 100\n",
    "        TEST_PRINT_INTERVAL = PRINT_INTERVAL*5\n",
    "\n",
    "    if FLAGS.log:\n",
    "        train_writer = tf.summary.FileWriter(FLAGS.logdir + '/' + exp_string, sess.graph)\n",
    "    print('Done initializing, starting training.')\n",
    "    prelosses, postlosses = [], []\n",
    "\n",
    "    num_classes = data_generator.num_classes # for classification, 1 otherwise\n",
    "    multitask_weights, reg_weights = [], []\n",
    "\n",
    "    for itr in range(resume_itr, FLAGS.pretrain_iterations + FLAGS.metatrain_iterations):\n",
    "        feed_dict = {}\n",
    "        if 'generate' in dir(data_generator):\n",
    "            batch_x, batch_y, amp, phase = data_generator.generate()\n",
    "\n",
    "            if FLAGS.baseline == 'oracle':\n",
    "                batch_x = np.concatenate([batch_x, np.zeros([batch_x.shape[0], batch_x.shape[1], 2])], 2)\n",
    "                for i in range(FLAGS.meta_batch_size):\n",
    "                    batch_x[i, :, 1] = amp[i]\n",
    "                    batch_x[i, :, 2] = phase[i]\n",
    "\n",
    "            inputa = batch_x[:, :num_classes*FLAGS.update_batch_size, :]\n",
    "            labela = batch_y[:, :num_classes*FLAGS.update_batch_size, :]\n",
    "            inputb = batch_x[:, num_classes*FLAGS.update_batch_size:, :] # b used for testing\n",
    "            labelb = batch_y[:, num_classes*FLAGS.update_batch_size:, :]\n",
    "            feed_dict = {model.inputa: inputa, model.inputb: inputb,  model.labela: labela, model.labelb: labelb}\n",
    "\n",
    "        if itr < FLAGS.pretrain_iterations:\n",
    "            input_tensors = [model.pretrain_op]\n",
    "        else:\n",
    "            input_tensors = [model.metatrain_op]\n",
    "\n",
    "        if (itr % SUMMARY_INTERVAL == 0 or itr % PRINT_INTERVAL == 0):\n",
    "            input_tensors.extend([model.summ_op, model.total_loss1, model.total_losses2[FLAGS.num_updates-1]])\n",
    "            if model.classification:\n",
    "                input_tensors.extend([model.total_accuracy1, model.total_accuracies2[FLAGS.num_updates-1]])\n",
    "\n",
    "        result = sess.run(input_tensors, feed_dict)\n",
    "\n",
    "        if itr % SUMMARY_INTERVAL == 0:\n",
    "            prelosses.append(result[-2])\n",
    "            if FLAGS.log:\n",
    "                train_writer.add_summary(result[1], itr)\n",
    "            postlosses.append(result[-1])\n",
    "\n",
    "        if (itr!=0) and itr % PRINT_INTERVAL == 0:\n",
    "            if itr < FLAGS.pretrain_iterations:\n",
    "                print_str = 'Pretrain Iteration ' + str(itr)\n",
    "            else:\n",
    "                print_str = 'Iteration ' + str(itr - FLAGS.pretrain_iterations)\n",
    "            print_str += ': ' + str(np.mean(prelosses)) + ', ' + str(np.mean(postlosses))\n",
    "            print(print_str)\n",
    "            prelosses, postlosses = [], []\n",
    "\n",
    "        if (itr!=0) and itr % SAVE_INTERVAL == 0:\n",
    "            saver.save(sess, FLAGS.logdir + '/' + exp_string + '/model' + str(itr))\n",
    "\n",
    "        # sinusoid is infinite data, so no need to test on meta-validation set.\n",
    "        if (itr!=0) and itr % TEST_PRINT_INTERVAL == 0 and FLAGS.datasource !='sinusoid':\n",
    "            if 'generate' not in dir(data_generator):\n",
    "                feed_dict = {}\n",
    "                if model.classification:\n",
    "                    input_tensors = [model.metaval_total_accuracy1, model.metaval_total_accuracies2[FLAGS.num_updates-1], model.summ_op]\n",
    "                else:\n",
    "                    input_tensors = [model.metaval_total_loss1, model.metaval_total_losses2[FLAGS.num_updates-1], model.summ_op]\n",
    "            else:\n",
    "                batch_x, batch_y, amp, phase = data_generator.generate(train=False)\n",
    "                inputa = batch_x[:, :num_classes*FLAGS.update_batch_size, :]\n",
    "                inputb = batch_x[:, num_classes*FLAGS.update_batch_size:, :]\n",
    "                labela = batch_y[:, :num_classes*FLAGS.update_batch_size, :]\n",
    "                labelb = batch_y[:, num_classes*FLAGS.update_batch_size:, :]\n",
    "                feed_dict = {model.inputa: inputa, model.inputb: inputb,  model.labela: labela, model.labelb: labelb, model.meta_lr: 0.0}\n",
    "                if model.classification:\n",
    "                    input_tensors = [model.total_accuracy1, model.total_accuracies2[FLAGS.num_updates-1]]\n",
    "                else:\n",
    "                    input_tensors = [model.total_loss1, model.total_losses2[FLAGS.num_updates-1]]\n",
    "\n",
    "            result = sess.run(input_tensors, feed_dict)\n",
    "            print('Validation results: ' + str(result[0]) + ', ' + str(result[1]))\n",
    "\n",
    "    saver.save(sess, FLAGS.logdir + '/' + exp_string +  '/model' + str(itr))\n",
    "\n",
    "# calculated for omniglot\n",
    "NUM_TEST_POINTS = 600\n",
    "\n",
    "def test(model, saver, sess, exp_string, data_generator, test_num_updates=None):\n",
    "    num_classes = data_generator.num_classes # for classification, 1 otherwise\n",
    "\n",
    "    np.random.seed(1)\n",
    "    random.seed(1)\n",
    "\n",
    "    metaval_accuracies = []\n",
    "\n",
    "    for _ in range(NUM_TEST_POINTS):\n",
    "        if 'generate' not in dir(data_generator):\n",
    "            feed_dict = {}\n",
    "            feed_dict = {model.meta_lr : 0.0}\n",
    "        else:\n",
    "            batch_x, batch_y, amp, phase = data_generator.generate(train=False)\n",
    "\n",
    "            if FLAGS.baseline == 'oracle': # NOTE - this flag is specific to sinusoid\n",
    "                batch_x = np.concatenate([batch_x, np.zeros([batch_x.shape[0], batch_x.shape[1], 2])], 2)\n",
    "                batch_x[0, :, 1] = amp[0]\n",
    "                batch_x[0, :, 2] = phase[0]\n",
    "\n",
    "            inputa = batch_x[:, :num_classes*FLAGS.update_batch_size, :]\n",
    "            inputb = batch_x[:,num_classes*FLAGS.update_batch_size:, :]\n",
    "            labela = batch_y[:, :num_classes*FLAGS.update_batch_size, :]\n",
    "            labelb = batch_y[:,num_classes*FLAGS.update_batch_size:, :]\n",
    "\n",
    "            feed_dict = {model.inputa: inputa, model.inputb: inputb,  model.labela: labela, model.labelb: labelb, model.meta_lr: 0.0}\n",
    "\n",
    "        if model.classification:\n",
    "            result = sess.run([model.metaval_total_accuracy1] + model.metaval_total_accuracies2, feed_dict)\n",
    "        else:  # this is for sinusoid\n",
    "            result = sess.run([model.total_loss1] +  model.total_losses2, feed_dict)\n",
    "        metaval_accuracies.append(result)\n",
    "\n",
    "    metaval_accuracies = np.array(metaval_accuracies)\n",
    "    means = np.mean(metaval_accuracies, 0)\n",
    "    stds = np.std(metaval_accuracies, 0)\n",
    "    ci95 = 1.96*stds/np.sqrt(NUM_TEST_POINTS)\n",
    "\n",
    "    print('Mean validation accuracy/loss, stddev, and confidence intervals')\n",
    "    print((means, stds, ci95))\n",
    "\n",
    "    out_filename = FLAGS.logdir +'/'+ exp_string + '/' + 'test_ubs' + str(FLAGS.update_batch_size) + '_stepsize' + str(FLAGS.update_lr) + '.csv'\n",
    "    out_pkl = FLAGS.logdir +'/'+ exp_string + '/' + 'test_ubs' + str(FLAGS.update_batch_size) + '_stepsize' + str(FLAGS.update_lr) + '.pkl'\n",
    "    with open(out_pkl, 'wb') as f:\n",
    "        pickle.dump({'mses': metaval_accuracies}, f)\n",
    "    with open(out_filename, 'w') as f:\n",
    "        writer = csv.writer(f, delimiter=',')\n",
    "        writer.writerow(['update'+str(i) for i in range(len(means))])\n",
    "        writer.writerow(means)\n",
    "        writer.writerow(stds)\n",
    "        writer.writerow(ci95)\n",
    "\n",
    "def main():\n",
    "    if FLAGS.datasource == 'sinusoid':\n",
    "        if FLAGS.train:\n",
    "            test_num_updates = 5\n",
    "        else:\n",
    "            test_num_updates = 10\n",
    "    else:\n",
    "        if FLAGS.datasource == 'miniimagenet':\n",
    "            if FLAGS.train == True:\n",
    "                test_num_updates = 1  # eval on at least one update during training\n",
    "            else:\n",
    "                test_num_updates = 10\n",
    "        else:\n",
    "            test_num_updates = 10\n",
    "\n",
    "    if FLAGS.train == False:\n",
    "        orig_meta_batch_size = FLAGS.meta_batch_size\n",
    "        # always use meta batch size of 1 when testing.\n",
    "        FLAGS.meta_batch_size = 1\n",
    "\n",
    "    if FLAGS.datasource == 'sinusoid':\n",
    "        data_generator = DataGenerator(FLAGS.update_batch_size*2, FLAGS.meta_batch_size)\n",
    "    else:\n",
    "        if FLAGS.metatrain_iterations == 0 and FLAGS.datasource == 'miniimagenet':\n",
    "            assert FLAGS.meta_batch_size == 1\n",
    "            assert FLAGS.update_batch_size == 1\n",
    "            data_generator = DataGenerator(1, FLAGS.meta_batch_size)  # only use one datapoint,\n",
    "        else:\n",
    "            if FLAGS.datasource == 'miniimagenet': # TODO - use 15 val examples for imagenet?\n",
    "                if FLAGS.train:\n",
    "                    data_generator = DataGenerator(FLAGS.update_batch_size+15, FLAGS.meta_batch_size)  # only use one datapoint for testing to save memory\n",
    "                else:\n",
    "                    data_generator = DataGenerator(FLAGS.update_batch_size*2, FLAGS.meta_batch_size)  # only use one datapoint for testing to save memory\n",
    "            else:\n",
    "                data_generator = DataGenerator(FLAGS.update_batch_size*2, FLAGS.meta_batch_size)  # only use one datapoint for testing to save memory\n",
    "\n",
    "\n",
    "    dim_output = data_generator.dim_output\n",
    "    if FLAGS.baseline == 'oracle':\n",
    "        assert FLAGS.datasource == 'sinusoid'\n",
    "        dim_input = 3\n",
    "        FLAGS.pretrain_iterations += FLAGS.metatrain_iterations\n",
    "        FLAGS.metatrain_iterations = 0\n",
    "    else:\n",
    "        dim_input = data_generator.dim_input\n",
    "\n",
    "    if FLAGS.datasource == 'miniimagenet' or FLAGS.datasource == 'omniglot':\n",
    "        tf_data_load = True\n",
    "        num_classes = data_generator.num_classes\n",
    "\n",
    "        if FLAGS.train: # only construct training model if needed\n",
    "            random.seed(5)\n",
    "            image_tensor, label_tensor = data_generator.make_data_tensor()\n",
    "            inputa = tf.slice(image_tensor, [0,0,0], [-1,num_classes*FLAGS.update_batch_size, -1])\n",
    "            inputb = tf.slice(image_tensor, [0,num_classes*FLAGS.update_batch_size, 0], [-1,-1,-1])\n",
    "            labela = tf.slice(label_tensor, [0,0,0], [-1,num_classes*FLAGS.update_batch_size, -1])\n",
    "            labelb = tf.slice(label_tensor, [0,num_classes*FLAGS.update_batch_size, 0], [-1,-1,-1])\n",
    "            input_tensors = {'inputa': inputa, 'inputb': inputb, 'labela': labela, 'labelb': labelb}\n",
    "\n",
    "        random.seed(6)\n",
    "        image_tensor, label_tensor = data_generator.make_data_tensor(train=False)\n",
    "        inputa = tf.slice(image_tensor, [0,0,0], [-1,num_classes*FLAGS.update_batch_size, -1])\n",
    "        inputb = tf.slice(image_tensor, [0,num_classes*FLAGS.update_batch_size, 0], [-1,-1,-1])\n",
    "        labela = tf.slice(label_tensor, [0,0,0], [-1,num_classes*FLAGS.update_batch_size, -1])\n",
    "        labelb = tf.slice(label_tensor, [0,num_classes*FLAGS.update_batch_size, 0], [-1,-1,-1])\n",
    "        metaval_input_tensors = {'inputa': inputa, 'inputb': inputb, 'labela': labela, 'labelb': labelb}\n",
    "    else:\n",
    "        tf_data_load = False\n",
    "        input_tensors = None\n",
    "\n",
    "    model = MAML(dim_input, dim_output, test_num_updates=test_num_updates)\n",
    "    if FLAGS.train or not tf_data_load:\n",
    "        model.construct_model(input_tensors=input_tensors, prefix='metatrain_')\n",
    "    if tf_data_load:\n",
    "        model.construct_model(input_tensors=metaval_input_tensors, prefix='metaval_')\n",
    "    model.summ_op = tf.summary.merge_all()\n",
    "\n",
    "    saver = loader = tf.train.Saver(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES), max_to_keep=10)\n",
    "\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    if FLAGS.train == False:\n",
    "        # change to original meta batch size when loading model.\n",
    "        FLAGS.meta_batch_size = orig_meta_batch_size\n",
    "\n",
    "    if FLAGS.train_update_batch_size == -1:\n",
    "        FLAGS.train_update_batch_size = FLAGS.update_batch_size\n",
    "    if FLAGS.train_update_lr == -1:\n",
    "        FLAGS.train_update_lr = FLAGS.update_lr\n",
    "\n",
    "    exp_string = 'cls_'+str(FLAGS.num_classes)+'.mbs_'+str(FLAGS.meta_batch_size) + '.ubs_' + str(FLAGS.train_update_batch_size) + '.numstep' + str(FLAGS.num_updates) + '.updatelr' + str(FLAGS.train_update_lr)\n",
    "\n",
    "    if FLAGS.num_filters != 64:\n",
    "        exp_string += 'hidden' + str(FLAGS.num_filters)\n",
    "    if FLAGS.max_pool:\n",
    "        exp_string += 'maxpool'\n",
    "    if FLAGS.stop_grad:\n",
    "        exp_string += 'stopgrad'\n",
    "    if FLAGS.baseline:\n",
    "        exp_string += FLAGS.baseline\n",
    "    if FLAGS.norm == 'batch_norm':\n",
    "        exp_string += 'batchnorm'\n",
    "    elif FLAGS.norm == 'layer_norm':\n",
    "        exp_string += 'layernorm'\n",
    "    elif FLAGS.norm == 'None':\n",
    "        exp_string += 'nonorm'\n",
    "    else:\n",
    "        print('Norm setting not recognized.')\n",
    "\n",
    "    resume_itr = 0\n",
    "    model_file = None\n",
    "\n",
    "    tf.global_variables_initializer().run()\n",
    "    tf.train.start_queue_runners()\n",
    "\n",
    "    if FLAGS.resume or not FLAGS.train:\n",
    "        model_file = tf.train.latest_checkpoint(FLAGS.logdir + '/' + exp_string)\n",
    "        if FLAGS.test_iter > 0:\n",
    "            model_file = model_file[:model_file.index('model')] + 'model' + str(FLAGS.test_iter)\n",
    "        if model_file:\n",
    "            ind1 = model_file.index('model')\n",
    "            resume_itr = int(model_file[ind1+5:])\n",
    "            print(\"Restoring model weights from \" + model_file)\n",
    "            saver.restore(sess, model_file)\n",
    "\n",
    "    if FLAGS.train:\n",
    "        train(model, saver, sess, exp_string, data_generator, resume_itr)\n",
    "    else:\n",
    "        test(model, saver, sess, exp_string, data_generator, test_num_updates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnrecognizedFlagError",
     "evalue": "Unknown command line flag 'f'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnrecognizedFlagError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-8dc4547d147a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasource\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'sinusoid'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mtest_num_updates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/anly580/lib/python3.8/site-packages/tensorflow/python/platform/flags.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# a flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_parsed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m       \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/anly580/lib/python3.8/site-packages/absl/flags/_flagvalues.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, argv, known_only)\u001b[0m\n\u001b[1;32m    652\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munknown_flags\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m       \u001b[0msuggestions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_helpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_flag_suggestions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m       raise _exceptions.UnrecognizedFlagError(\n\u001b[0m\u001b[1;32m    655\u001b[0m           name, value, suggestions=suggestions)\n\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnrecognizedFlagError\u001b[0m: Unknown command line flag 'f'"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anly580",
   "language": "python",
   "name": "anly580"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
